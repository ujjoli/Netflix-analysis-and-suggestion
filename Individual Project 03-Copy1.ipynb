{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Netflix Analysis and Suggestions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ujjwal Oli"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Netflix is popular online streaming service that offers a wide variety of award-winning TV shows, movies, anime, documentaries, and more on thousands of internet-connected devices. It is being popular as you can watch as much as you want, whenever you want without a single commercial – all for one low monthly price. With the streaming service, people are getting choice to watch movies online at home with their family. I mean who does not like this service, right? But, along with this service, are the contents of Netflix qualitative (highly rated)? People have their own choices, so is Netflix providing abundant variety of choices of contents? What genre are most popular in netflix? Keeping this question in mind, I am sure that some people love Netflix, and some don’t. So what is actually netflix missing? How can it increase the likeable contents based on the popularity of its present context? Can I recommend some contents of netflix? How does recommendation system work? What if we can predict the rating for netflix and even provide some suggestion in regard to the content it has based on directors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#<img src=\"netflix.jpg\">\n",
    "#from PIL import Image\n",
    "\n",
    "#image = Image.open(\"netflix.jpg\")\n",
    "#print(image.size)\n",
    "#n_image = image.resize((800, 400))\n",
    "#n_image.save('image_800.jpg')\n",
    "#print(n_image.size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='image_800.jpg'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, to answer these questions, my project is going to analyze the data from netflix. I am going to start with general analysis and come up with the suggestion and recommendation model for netflix and users. To start with the project, for one part of the data I used uNogs: RapidAPI to collect the data from API and downloaded the data as json format. Once I had json file, I converted it into csv and read the CSV files in pandas dataframe to perform processing, fitering, analysis and visualizations. Similarly, for another part of the data, I used kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "import csv\n",
    "import pandas as pd\n",
    "import time\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from os import path               #implements some useful functions on pathnames \n",
    "\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator   #wordcloud generator with stopwords\n",
    "from textblob import TextBlob       #supports natural language processing (NLP) tasks such as sentiment analysis\n",
    "import nltk                         #nltk is a Python package for natural language processing.\n",
    "from nltk import word_tokenize     #Tokenizers divide strings into lists of substrings.  For example,\n",
    "                                    #tokenizers can be used to find the words and punctuation in a string\n",
    "    \n",
    "nltk.download('punkt')             # The NLTK data package includes a pre-trained Punkt tokenizer for English\n",
    "\n",
    "\n",
    "from nltk.corpus import stopwords       #NLTK(Natural Language Toolkit) in python has a list of stopwords\n",
    "nltk.download('stopwords')               # The NLTK data package includes list of stopwords\n",
    "\n",
    "import re                            # module provides regular expression matching operations\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer     #Convert a collection of text documents to a matrix of token counts\n",
    "import operator                                                 #module defines functions that correspond to the concept of getters. \n",
    "from sklearn.model_selection import train_test_split   #Split arrays or matrices into random train and test subsets\n",
    "from sklearn import metrics\n",
    "from scipy import spatial                          #for distance computations in various metrics\n",
    "\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "matplotlib.style.use('ggplot')\n",
    "\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "import plotly.offline as pyo              #with this I will be able to plot charts offline\n",
    "import plotly.graph_objects as go            #using plotly graph objects\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#from nltk.probability import FreqDist  #this is for frequency distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing and reading files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 =  pd.read_csv('data1_file.csv')\n",
    "df2 = pd.read_csv('data2_file.csv')\n",
    "n_data = pd.read_csv(\"netflix_titles.csv\")\n",
    "print(df1.shape)\n",
    "print(df2.shape)\n",
    "print(n_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering and processing dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merging two dataframes\n",
    "netflix_data = df1.append(df2)\n",
    "\n",
    "netflix_data['country']=netflix_data['country'].fillna('usa')\n",
    "netflix_data.isnull().sum()\n",
    "\n",
    "#checking the data column values to see if we have any null values\n",
    "\n",
    "print(n_data.isnull().sum())\n",
    "print(n_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping some unwanted columns and renaming column name\n",
    "netflix_dat = netflix_data.drop(columns=['image','largeimage','download','imdbid'], axis= 1)\n",
    "netflix_dat.rename(columns={'synopsis':'description','released':'release_year'}, inplace =True)\n",
    "netflix_dat\n",
    "n_data.rename(columns={'show_id':'netflixid'}, inplace =True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check for duplicates\n",
    "\n",
    "n_data_filter =n_data.drop_duplicates(subset=['netflixid','title'])\n",
    "netflix_data_filter =netflix_dat.drop_duplicates(subset=['netflixid','title'])\n",
    "\n",
    "print(netflix_data_filter.duplicated().any()) #again checking if any duplicates left\n",
    "print(n_data_filter.duplicated().any()) # checking if any duplicates left for another dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My dataframes are:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_data_filter.head(2)\n",
    "netflix_data_filter.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## extracting dates from the date columns and making ne columns\n",
    "n_data_filter[\"date_added\"] = pd.to_datetime(n_data_filter['date_added'])\n",
    "n_data_filter['added_year'] = n_data_filter['date_added'].dt.year\n",
    "n_data_filter['added_month'] = n_data_filter['date_added'].dt.month\n",
    "n_data_filter\n",
    "\n",
    "#netflix_dat[\"unogsdate\"] = pd.to_datetime(netflix_dat['unogsdate'])  #this gave me error so I had to look into the the column\n",
    "#netflix_dat[\"date_filter\"] = netflix_dat[\"unogsdate\"].apply(lambda x: len(x))\n",
    "#netflix_dat_extra = netflix_dat[netflix_dat[\"date_filter\"] != 10]\n",
    "#netflix_dat = netflix_dat.drop(3200)\n",
    "\n",
    "netflix_data_filter[\"unogsdate\"] = pd.to_datetime(netflix_data_filter['unogsdate']) \n",
    "netflix_data_filter['added_year'] = netflix_data_filter['unogsdate'].dt.year\n",
    "netflix_data_filter['added_month'] = netflix_data_filter['unogsdate'].dt.month\n",
    "netflix_data_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#selecting specific columns \n",
    "rating_view = netflix_data_filter.loc[:,['netflixid','rating']]\n",
    "rating_view.rename(columns={'rating':'rating_num'}, inplace =True)\n",
    "#print(rating_view.shape)\n",
    "\n",
    "#merging data frame \n",
    "merge_data = pd.merge(n_data_filter, rating_view, on='netflixid', how='inner')\n",
    "\n",
    "#print(merge_data.shape)  #checking the rows and columns of the df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_list = merge_data.netflixid.to_list()  #making a list of netflixid\n",
    "#print(len(merged_list))#530\n",
    "rating_data_list = rating_view.netflixid.to_list() #making a list of netflixid from another dataframe\n",
    "#print(len(rating_data_list))#764\n",
    "\n",
    "data_to_add =[]  #making empty list\n",
    "\n",
    "for i in rating_data_list:    #using for loop to to get the netflixid that is not in the merged dataframe\n",
    "    if i not in merged_list:\n",
    "        data_to_add.append(i)   #appending list with the missed netflixid\n",
    "\n",
    "#data_to_add  #prints list of netflixid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting index \n",
    "netflix_data_filter_set= netflix_data_filter.set_index(\"netflixid\")\n",
    "\n",
    "#making a new df and resetting index\n",
    "data_add_df = netflix_data_filter_set.loc[data_to_add]\n",
    "data_add_df_set = data_add_df.reset_index()\n",
    "data_add_df_set.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#renaming columns \n",
    "add_df_rename =data_add_df_set.rename(columns={'rating':'rating_num',\n",
    "                                              'runtime':'duration'})\n",
    "\n",
    "#merging two dataframes to include all netflixid\n",
    "rating_data_overall = merge_data.append(add_df_rename, ignore_index = True) \n",
    "rating_data_overall.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding count of high-rated movie and low-rated movie\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_count_rated = rating_data_overall.copy()   #making copy of the dataframe\n",
    "copy_count_rated.rating_num = copy_count_rated.rating_num.fillna(0)  #filling null values with zero for easy processing\n",
    "#copy_count_rated.rating_num.value_counts()           #counts value for specific columns \n",
    "copy_count_rated['rating_standard'] = 'N/A'  #making a new column in dataframe\n",
    "copy_count_rated.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my comparison standard for high-rating is 7.0 so any movie/show that is equal or more than 7.0 is high-rated and \n",
    "#any movie/show that is less than 7.0 is low-rated.\n",
    "\n",
    "#using for loop to make a new column and specify the movie/show as high-rated or low-rated\n",
    "for each in range(len(copy_count_rated.rating_num)):\n",
    "    if copy_count_rated.rating_num[each] >=7.0:\n",
    "        copy_count_rated['rating_standard'][each] = 'High-rated'\n",
    "    elif copy_count_rated.rating_num[each] <7.0: #and copy_count_rated.rating_num[each] >=1:\n",
    "        copy_count_rated['rating_standard'][each] = 'Low-rated'\n",
    "        \n",
    "print(copy_count_rated.rating_standard.value_counts())   #printing total number of counts of high-rated and low-rated movie\n",
    "copy_count_rated.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotly chart - Bar chart to see the count of movies and shows in our data\n",
    "#x-axis will show the content type and y-axis will show the count of content\n",
    "\n",
    "color = ['steelblue','firebrick']\n",
    "data = [go.Bar(x=['High-rated','Low-rated'],\n",
    "y=[copy_count_rated.loc[copy_count_rated['rating_standard']=='High-rated'].shape[0],\n",
    "   copy_count_rated.loc[copy_count_rated['rating_standard']=='Low-rated'].shape[0]],\n",
    "    #marker_color =copy_count_rated['rating_num']\n",
    "   marker=dict(color=color) \n",
    "    \n",
    ")]\n",
    "#create the layout of the chart by defining titles for chart, x-axis and y-axis\n",
    "layout = go.Layout(title='Netflix content rating analysis',\n",
    "            xaxis=dict(title='Type of ratings'),\n",
    "            yaxis=dict(title='Total no. of ratings'),\n",
    "            height =500,\n",
    "            width = 700)\n",
    "\n",
    "#Imbed data and layout into charts figure using Figure function\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "#Use plot function of plotly to visualize the data\n",
    "fig.show()\n",
    "#fig.write_html(\"path/to/file.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making copy to new dataframe so that the edit do not change the original dataframe\n",
    "copy_count_rated_country =copy_count_rated.copy()\n",
    "\n",
    "#replacing abbreviation of country names with their full name\n",
    "copy_count_rated_country = copy_count_rated_country.replace({'country': {'gb':'United Kingdom', 'ar':'Argentina', 'hk':'Hongkong','be':'Belgium','hu':'Hungary','cz':'cezhrepublic',\n",
    "                  'de':'Germany','jp':'Japan','se':'Sweden','ru':'Russia','au':'Australia','nl':'Netherland','usa':'United States','in':'India',\n",
    "                  'lt':'Luthvania','br':'Brazil','mx':'Mexico','sg':'Singapore','fr':'France','kr':'South Korea','it':'Italy'}})\n",
    "copy_count_rated_country.head(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#acessing specific columns\n",
    "rating_analysis_country_filter = copy_count_rated_country.loc[:,['netflixid','type','title','country','rating','duration','listed_in','description','rating_num','rating_standard']]\n",
    "print(rating_analysis_country_filter.shape)\n",
    "\n",
    "rating_analysis_country_filter.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#country_high_rating_count = x\n",
    "# in country column there are more than one country in a row, so making separate rows for each country \n",
    "#so creating a function and using for loop to separate the each country in different rows\n",
    "def get_each_country(x):\n",
    "    ids_list =[]\n",
    "    countries_df=pd.DataFrame()   #making a new empty dataframe\n",
    "    x['country'] = x['country'].astype(str)   #making sure that all values in country columns are in string format\n",
    "    for i in range(len(x)):   #using for loop\n",
    "        nid = x.iloc[i,0]                     #getting particular values\n",
    "        typ = x.iloc[i,1]                       #getting particular values\n",
    "        titl=x.iloc[i,2]                     #getting particular values\n",
    "        value = x.iloc[i,3]                     #getting particular values\n",
    "        rating_typ = x.iloc[i,4]                  #getting particular values\n",
    "        dura = x.iloc[i,5]\n",
    "        genre = x.iloc[i,6]\n",
    "        desc = x.iloc[i,7]\n",
    "        rate = x.iloc[i,8]\n",
    "        rate_std = x.iloc[i,9]                       #getting particular values\n",
    "        if ',' in value:                             #checking if comma is in the specific row value \n",
    "            splitted= value.split(',')         #splitting the value  separting with commas\n",
    "            ids_list.append(nid)          #getting id of that value\n",
    "            if len(splitted)==2:          #checking how many values were separated with commas (condition with 2 values)\n",
    "                #making a list of series to append later in new dataframe\n",
    "                listOfSeries = [pd.Series([nid, typ, titl, splitted[0].strip(),rating_typ, dura,genre,desc,rate,rate_std], index=x.columns ) , #making a complete row\n",
    "                        pd.Series([nid, typ, titl, splitted[1].strip(),rating_typ, dura,genre,desc,rate,rate_std], index=x.columns )]          #making a complete row\n",
    "            elif len(splitted) ==3:         # 3 country names separated with commas\n",
    "                listOfSeries = [pd.Series([nid, typ, titl, splitted[0].strip(),rating_typ, dura,genre,desc,rate,rate_std], index=x.columns ) ,#making a complete row\n",
    "                        pd.Series([nid, typ, titl, splitted[1].strip(),rating_typ, dura,genre,desc,rate,rate_std], index=x.columns ),#making a complete row\n",
    "                        pd.Series([nid, typ, titl, splitted[2].strip(),rating_typ, dura,genre,desc,rate,rate_std], index=x.columns )]#making a complete row\n",
    "            else:          #more tha three countries \n",
    "                listOfSeries = [pd.Series([nid, typ, titl, splitted[0].strip(),rating_typ, dura,genre,desc,rate,rate_std], index=x.columns ) , #making a complete row\n",
    "                        pd.Series([nid, typ, titl, splitted[1].strip(),rating_typ, dura,genre,desc,rate,rate_std], index=x.columns ),#making a complete row\n",
    "                        pd.Series([nid, typ, titl, splitted[2].strip(),rating_typ, dura,genre,desc,rate,rate_std], index=x.columns ), #making a complete row\n",
    "                        pd.Series([nid, typ, titl, splitted[3].strip(),rating_typ, dura,genre,desc,rate,rate_std], index=x.columns )]#making a complete row\n",
    "\n",
    "            countries_df = countries_df.append(listOfSeries , ignore_index=True) #appending all these rows in a new dataframe\n",
    "\n",
    "    #print(len(id_list))\n",
    "    #print(len(country_df))\n",
    "    #print(country_df)\n",
    "\n",
    "\n",
    "    countries_index = x.set_index('netflixid') #setting index in the dataframe that is passed to this funntion\n",
    "    #country_index\n",
    "\n",
    "    for each in range(len(x)):                \n",
    "        if x.iloc[each][0] in ids_list:    #checking if the each id is in the appended list above\n",
    "            val = x.iloc[each][0]            #getting id as val\n",
    "            countries_index = countries_index.drop(val,axis=0) #dropping the val(id)from the dataframe\n",
    "        else:\n",
    "            continue\n",
    "    countries_index  =   countries_index.reset_index() #resetting the index \n",
    "\n",
    "\n",
    "    countries_index = countries_index.append(countries_df, ignore_index =True)  #appending the index \n",
    "    return countries_index    #returning the dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rating_analysis_expand = get_each_country(rating_analysis_country_filter)  #calling above function to get each country rows separate\n",
    "#print(rating_analysis_expand.shape)\n",
    "rating_analysis_expand.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the rows that belongs to high-rated \n",
    "country_high_rating = rating_analysis_expand[rating_analysis_expand.rating_standard == 'High-rated']\n",
    "#getting with high-rated\n",
    "country_high_rating_count= country_high_rating.loc[:,['netflixid','country']]\n",
    "high_rating_by_country = country_high_rating_count.rename(columns={'country':'high-rating_country'})\n",
    "high_rating_by_country\n",
    "#counting countries that has more high-rated\n",
    "ccount_high = high_rating_by_country['high-rating_country'].value_counts()\n",
    "ccount_high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " #getting the rows that belongs to low-rated\n",
    "country_low_rating = rating_analysis_expand[rating_analysis_expand.rating_standard == 'Low-rated']\n",
    "country_low_rating_count= country_low_rating.loc[:,['netflixid','country']]\n",
    "#getting countries with low-rated and renaming the columns\n",
    "low_rating_by_country = country_low_rating_count.rename(columns={'country':'low-rating_country'})\n",
    "#counting the total number of low-rated movies in that country\n",
    "ccount_low = low_rating_by_country['low-rating_country'].value_counts()\n",
    "ccount_low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concating high -rated dataframe and lowrated dataframe\n",
    "rating_compare = pd.concat([ccount_high, ccount_low], axis=1)\n",
    "rating_compare =rating_compare.reset_index()\n",
    "rating_compare\n",
    "\n",
    "\n",
    "\n",
    "#renaming the columns of concatenated dataframe\n",
    "rating_compare_e =rating_compare.rename(columns={'index':'country',\n",
    "                                                    'high-rating_country':'High-rated',\n",
    "                                                    'low-rating_country':'Low-rated'})\n",
    "\n",
    "#using melt functionality to set the columna name as valuues\n",
    "rating_compare_edit = rating_compare_e.melt(id_vars=['country'], value_vars=['High-rated', 'Low-rated'])\n",
    "#rating_compare_edit.columns.values\n",
    "#rating_compare_edit.country.to_list()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#using iloc to get specific countries\n",
    "rating_compare_country = rating_compare_edit.iloc[[0,52, 53,1,54,2,55,3,56,4,57,5,58,6,59,7,60,12,64],:]\n",
    "rating_compare_country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting the data into sunburst using plotly\n",
    "fig = px.sunburst(rating_compare_country, path=['country', 'variable'], values='value', color='country',\n",
    "                  title='Analysis of Netflix ratings by country', height = 600, width =650)\n",
    "fig.show()\n",
    "#fig.write_html(\"try1 copy.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Popular Genres in these countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# getting a glance at types of genres we have in our dataframe\n",
    "rating_analysis_expand.listed_in.to_list()\n",
    "\n",
    "rating_analysis_expand.columns.values  #Getting column values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in our datafraem we noticed that we had many genres in each row so separating it to process and analyze it.\n",
    "#so using the function to separate the values by genre and making them individual rows\n",
    "def get_each_genre(x):\n",
    "    id_list =[]\n",
    "    genre_df=pd.DataFrame()    #creaing new dataframe\n",
    "    x['listed_in'] = x['listed_in'].astype(str)   #converting ito string format\n",
    "    for i in range(len(x)):       #using for loop\n",
    "        nid = x.iloc[i,0]  #accesing specific value\n",
    "        typ = x.iloc[i,1]      #accesing specific value\n",
    "        titl=x.iloc[i,2]         #accesing specific value\n",
    "        country = x.iloc[i,3]        #accesing specific value\n",
    "        rating_typ = x.iloc[i,4]#accesing specific value\n",
    "        dura = x.iloc[i,5]          #accesing specific value\n",
    "        value = x.iloc[i,6]         #accesing specific value\n",
    "        desc = x.iloc[i,7]          #accesing specific value\n",
    "        rate = x.iloc[i,8]          #accesing specific value\n",
    "        rate_std = x.iloc[i,9]       #accesing specific value\n",
    "        if ',' in value:        #checking for the comma in the value to split it accordingly\n",
    "            splitted= value.split(',')  #splotting th value\n",
    "            id_list.append(nid)\n",
    "            if len(splitted)==2:    #making a new series for more than two generes in each row\n",
    "                listOfSeries = [pd.Series([nid, typ, titl, country,rating_typ, dura,splitted[0].strip(),desc,rate,rate_std], index=x.columns ) ,\n",
    "                        pd.Series([nid, typ, titl, country,rating_typ, dura,splitted[1].strip(),desc,rate,rate_std], index=x.columns )]\n",
    "            elif len(splitted) ==3:   #making a new series for more than two generes in each row\n",
    "                listOfSeries = [pd.Series([nid, typ, titl, country,rating_typ, dura,splitted[0].strip(),desc,rate,rate_std], index=x.columns ) ,\n",
    "                        pd.Series([nid, typ, titl, country,rating_typ, dura,splitted[1].strip(),desc,rate,rate_std], index=x.columns ),\n",
    "                        pd.Series([nid, typ, titl, country,rating_typ, dura,splitted[2].strip(),desc,rate,rate_std], index=x.columns )]\n",
    "            \n",
    "\n",
    "            genre_df = genre_df.append(listOfSeries , ignore_index=True)  #appending the dataframe with new rows\n",
    "\n",
    "    #print(len(id_list))\n",
    "    #print(len(country_df))\n",
    "    #print(country_df)\n",
    "\n",
    "    \n",
    "   \n",
    "    \n",
    "    countries_genre_index = x.set_index('netflixid')  #setting the index\n",
    "    #countries_genre_index\n",
    "    id_list_final = set(id_list)     #removing duplicate items in the list by making it set type\n",
    "    \n",
    "    for each in id_list_final:\n",
    "        countries_genre_index = countries_genre_index.drop(each, axis = 0)  #dropping the rows that have been repeated in another dataframe\n",
    "        \n",
    "    \n",
    "    countries_genre_index  =   countries_genre_index.reset_index()  #resetting index \n",
    "    countries_genre_index = countries_genre_index.append(genre_df, ignore_index =True) #appending the new dataframe\n",
    "    return countries_genre_index #returning the cleaned dataframe \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_analysis_genre = get_each_genre(rating_analysis_expand)  #calling the function to process the genre columns\n",
    "rating_analysis_genre"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "rating_analysis_genre.listed_in.value_counts()\n",
    "rating_analysis_genre_Get = rating_analysis_genre.loc[rating_analysis_genre.rating_num == 0]\n",
    "rating_analysis_genre_Get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there are too many genres so processing them to major genres accordingly by replace functionality\n",
    "rating_analysis_genre['listed_in'] = rating_analysis_genre['listed_in'].replace(['Movie','Classic Movies',\n",
    "                                'Kid\\'s TV', 'Teens TV','Teen TV Shows','Kids\\' TV',\n",
    "                                'Stand-Up Comedy','TV Comedies','Stand-Up Comedy & Talk Shows',\n",
    "                                'TV Dramas','Docuseries','TV Action & Adventure','Crime TV Shows',\n",
    "                                'Children & Family Movies', 'Faith & Spirituality',\n",
    "                                'Independent Movies','LGBTQ Movies',\n",
    "                                'Thrillers','Horror','TV Horror','TV Thrillers',\n",
    "                                'TV Sci-Fi & Fantasy','Romantic Moives','Romantic TV Shows',\n",
    "                                'TV Mysteries','British TV Shows','Classic & Cult TV','Korean TV Shows','Reality TV',\n",
    "                        'Anime Features'],\n",
    "                        ['International Movies','International Movies',\n",
    "                        'Kid\\'s and Teen','Kid\\'s and Teen','Kid\\'s and Teen', 'Kid\\'s and Teen',\n",
    "                        'Comedies','Comedies','Comedies',\n",
    "                         'Dramas','Documentaries','Action & Adventure','Crime',\n",
    "                        'Family Movies','Family Movies',\n",
    "                        'Independent Movies and LGBTQ Movies','Independent Movies and LGBTQ Movies',\n",
    "                        'Thrillers & Horror','Thrillers & Horror','Thrillers & Horror','Thrillers & Horror',\n",
    "                        'Sci-Fi & Fantasy','Romantic','Romantic',\n",
    "                        'Other Shows','Other Shows','Other Shows','Other Shows','Other Shows',\n",
    "                        'Anime Series'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#also replacing movie type to Movie type to make the data similar and comparable\n",
    "rating_analysis_genre['type'] = rating_analysis_genre['type'].replace('movie','Movie')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using treemap to analyze the quality content of netflix in each country by genre\n",
    "fig = px.treemap(rating_analysis_genre, path=['country','listed_in','rating_standard'], \n",
    "                 title ='Content analysis of each country by genre',\n",
    "                  color='rating_num', hover_data=['rating_num'], \n",
    "                  color_continuous_scale='RdBu')\n",
    "fig.show()\n",
    "#fig.write_html(\"try2 copy.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Duration analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making a nes dataframe by selecting specific columns\n",
    "duration_analysis = rating_analysis_genre.loc[:,['netflixid','type','duration','rating_standard','rating_num']]\n",
    "# duration_analysis        #printing duration analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#getting specific rows where we find the movie because we are anlyzing the length of movie\n",
    "movie_analysis = duration_analysis.loc[(duration_analysis['type'] =='Movie')]\n",
    " \n",
    "#checking for the duplicates and drop the duplicated items in the dataframe\n",
    "movie_analysis_edited = movie_analysis.drop_duplicates(subset=['netflixid'])\n",
    "\n",
    "#splitting the duration column to actual duration and to units of duration measurrement(mins)\n",
    "movie_analysis_edited[['duration_in_mins','duration_units']] = movie_analysis_edited.duration.str.split(\" \",expand=True,)\n",
    "\n",
    "#droping nan values\n",
    "movie_analysis_edited = movie_analysis_edited.dropna()\n",
    "#but all the values in duration column was not in same format so it wasnot easily splitted\n",
    "\n",
    "#dataframe of high-rated movies with their duration\n",
    "high_movie_analysis_edited = movie_analysis_edited[movie_analysis_edited.rating_standard == 'High-rated']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#duration column had different formatted values so used regular expression to ge the float numbers of out it.\n",
    "s= movie_analysis_edited.duration.to_list()  #making a list\n",
    "lis=[]\n",
    "to_sum=[]\n",
    "for each in s:\n",
    "    counter = 0\n",
    "    for each in re.findall(r'\\d+',each):   #finding all the values using regular expression\n",
    "        if counter == 0:  #setting the condition\n",
    "            x =float(each)    #converting it to float\n",
    "            if x ==1: \n",
    "                x = 60\n",
    "            elif x ==2:\n",
    "                x =120\n",
    "            sume = x   #assigning the value\n",
    "            #print(f\"x is{x}\")\n",
    "            #print(f\"sume is {sume}\")\n",
    "            \n",
    "        else:\n",
    "            y = float(each)   #converting to float\n",
    "            #print(y)\n",
    "            sume=sume+y\n",
    "            #print(f\"sume is {sume}\")\n",
    "        counter = counter +1\n",
    "        #print(counter)\n",
    "    lis.append(sume)  #appendig the list\n",
    "\n",
    "movie_analysis_edited['duration_in_min'] = lis  #making a new column with the appended list to get the float numbers that represnet the duration\n",
    "\n",
    "#movie_analysis_edited   #prints the dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting the histogram of distribution of movies with both high-standard and low-standard to analyze their distribution pattern\n",
    "fig = px.histogram(movie_analysis_edited, x=\"duration_in_min\", color=\"rating_standard\", \n",
    "                   nbins=18,title='Distribution of duration of movies by rating standard', marginal=\"box\") # can be `box`, `violin`)\n",
    "                   \n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting histogram to analyze the distribution of duration of high-rated movies\n",
    "fig = px.histogram(high_movie_analysis_edited, x=\"duration_in_mins\", \n",
    "                   title='Distribution of duration of high-rated movies', marginal=\"box\",nbins=18)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding co-relation between duration and rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#analyzing the realation of duration and rating\n",
    "# to see if increase or decrease in length of movie would impact the rating\n",
    "fig = px.scatter(movie_analysis_edited, x=\"duration_in_min\", y=\"rating_num\", color=\"rating_standard\",\n",
    "                 title = 'Analysis of co-relation between rating and duration',\n",
    "                  hover_data=['type'])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finding co-relation to see if the duration variable and rating variable are co-related\n",
    "from scipy.stats import pearsonr \n",
    "  \n",
    "# Convert dataframe into series \n",
    "list1 = movie_analysis_edited['duration_in_min'] \n",
    "list2 = movie_analysis_edited['rating_num'] \n",
    "  \n",
    "# Apply the pearsonr() \n",
    "corr, _ = pearsonr(list1, list2) \n",
    "print('Pearsons correlation: %.3f' % corr)\n",
    "\n",
    "correlation = list1.corr(list2)\n",
    "correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rating class popularity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_data_filter.head(1)   #taking  a glance at the dataframe\n",
    "\n",
    "#accessing specifc columns \n",
    "data_rating_class= n_data_filter.loc[:,['netflixid','rating','country']]\n",
    "data_rating_class.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets see which type of rating movie or TV show the netflix has the most\n",
    "rating_value_counts = data_rating_class.rating.value_counts()\n",
    "rating_value_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting colors\n",
    "color = ['salmon', 'firebrick', 'aqua', 'mediumorchid', 'orangered',\n",
    "           'limegreen', 'gold', 'tomato', 'magenta', 'blue',\n",
    "            'blueviolet', 'brown', 'burlywood', 'cadetblue',\n",
    "            'chartreuse']\n",
    "#using bar plotly\n",
    "data = [go.Bar(x=['TV-MA','TV-14','TV-PG','R','PG-13','NR','PG','TV-Y7','TV-G','TV-Y','TV-Y7-F7','G','UR','NC-17'],\n",
    "\n",
    "#setting y to be value count of each rating type\n",
    "y=[data_rating_class.loc[data_rating_class['rating']=='TV-MA'].shape[0],  \n",
    "   data_rating_class.loc[data_rating_class['rating']=='TV-14'].shape[0],\n",
    "   data_rating_class.loc[data_rating_class['rating']=='TV-PG'].shape[0],\n",
    "    data_rating_class.loc[data_rating_class['rating']=='R'].shape[0],\n",
    "    data_rating_class.loc[data_rating_class['rating']=='PG-13'].shape[0],\n",
    "    data_rating_class.loc[data_rating_class['rating']=='NR'].shape[0],\n",
    "    data_rating_class.loc[data_rating_class['rating']=='PG'].shape[0],\n",
    "               data_rating_class.loc[data_rating_class['rating']=='TV-Y7'].shape[0],\n",
    "               data_rating_class.loc[data_rating_class['rating']=='TV-G'].shape[0],\n",
    "               data_rating_class.loc[data_rating_class['rating']=='TV-Y'].shape[0],\n",
    "               data_rating_class.loc[data_rating_class['rating']=='TV-Y7-F7'].shape[0],\n",
    "               data_rating_class.loc[data_rating_class['rating']=='G'].shape[0],\n",
    "               data_rating_class.loc[data_rating_class['rating']=='UR'].shape[0],\n",
    "               data_rating_class.loc[data_rating_class['rating']=='NC-17'].shape[0]],\n",
    "   marker=dict(color=color) \n",
    "    \n",
    ")]\n",
    "#create the layout of the chart by defining titles for chart, x-axis and y-axis\n",
    "layout = go.Layout(title='Netflix content rating analysis',\n",
    "            xaxis=dict(title='Type of ratings'),\n",
    "            yaxis=dict(title='Total no. of ratings'),\n",
    "            height =500,\n",
    "            width = 700)\n",
    "\n",
    "#embedding data and layout into charts figure using Figure function\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "#Use plot function of plotly to visualize the data\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_analysis_expand.head(2)  #printing two rows of data frame to see what the data frame looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making a copy of dataframe\n",
    "rating_analysis_by_country = rating_analysis_expand.copy()\n",
    "\n",
    "#filling empty values with not available for ease of analysis\n",
    "rating_analysis_by_country=rating_analysis_by_country.fillna('N/A')\n",
    "\n",
    "#checking if the dataframe now has null values\n",
    "rating_analysis_by_country.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting sunbrust plotly to see the type of rating type by the country\n",
    "fig = px.sunburst(rating_analysis_by_country, path=['country', 'rating'],  color='country',\n",
    "                  title='Analysis of Netflix ratings by country', height = 600, width =700)\n",
    "fig.show()\n",
    "#fig.write_html(\"try3 copy.html\")  #saving figure as html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### now that we have analyzed the rating type, since we also have data of title, cast, directors lets analyze these pieces of incormation and see if we can come up with some insights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## High-rated titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coming up with some popular words among all titles for high-rated movie/shows\n",
    "country_high_rating.head(1)\n",
    "#country_high_rating[country_high_rating.duplicated(['netflixid'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#opening image in numpy array format to shape word cloud in this saved image\n",
    "#to do that we need to check the intensity of pixels, which acn be done by opening image in numpy format\n",
    "TV_mask = np.array(Image.open(\"TV.jpg\")) \n",
    "#TV_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mask has to be in 255 pixels to use it in word cloud \n",
    "# our mask is in correct form so, let's create text for wordcloud\n",
    "text = \" \".join(country_high_rating['title'])\n",
    "\n",
    "#creating a word cloud image\n",
    "wc = WordCloud(background_color='black',max_words =500, mask = TV_mask, \n",
    "               contour_width =3, contour_color = 'red')\n",
    "\n",
    "#generate a wordcloud\n",
    "wc.generate(text)\n",
    "\n",
    "#show\n",
    "plt.figure(figsize=[20,7])\n",
    "plt.title(\"Wordcloud for popoular words in titles\", fontsize =30)\n",
    "plt.imshow(wc)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## High-rated directors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similarly some popular directors from high rated movies/shows\n",
    "\n",
    "#accessing specific columns\n",
    "rated_directorANDactor = copy_count_rated.loc[:,['director','cast','rating_standard','title']]\n",
    "\n",
    "#accessing specific rows\n",
    "high_rated_directorANDactor = rated_directorANDactor.loc[rated_directorANDactor.rating_standard == 'High-rated']\n",
    "\n",
    "#dropping nan values\n",
    "high_rated_director = high_rated_directorANDactor.director.dropna()\n",
    "#high_rated_director"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# our mask is in correct form so, let's create text for wordcloud\n",
    "text = \" \".join(high_rated_director)\n",
    "\n",
    "#creating a word cloud image with maximum words of 200 \n",
    "wc = WordCloud(background_color='black',max_words =200, mask = TV_mask, \n",
    "               contour_width =3, contour_color = 'red')\n",
    "\n",
    "#generate a wordcloud\n",
    "wc.generate(text)\n",
    "\n",
    "#show\n",
    "plt.figure(figsize=[25,8])\n",
    "plt.title(\"Popular directors\", fontsize =25)\n",
    "plt.imshow(wc)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Most common 4 director names with High rated movies are:\n",
    "    1. Shannon Hartman\n",
    "    2. Jay Karas\n",
    "    3. Lilly Wachowski, Lana Wachowski\n",
    "    4. Mike Clattenburg\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#having some information related with director Shannon Hartman\n",
    "high_rated_directorANDactor[high_rated_directorANDactor['director'].str.match('^Shannon Hartman*')== True][:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Pattern for content description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I would also like to analyze if there is any specific pattern in description of movie/show. Can we tell which genre will that movie/show belong by looking at the descritption? If yes, are there any commonalities we can find in that gerne's?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_analysis_genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting speciifc columns needed for analysis\n",
    "rating_analysis_genre_edit =rating_analysis_genre.loc[:,['netflixid','listed_in','description','rating_standard']]\n",
    "\n",
    "#looking for duplicates and keeping the first records if there are any duplicates\n",
    "analysis_genre = rating_analysis_genre_edit.drop_duplicates(subset='netflixid',keep= 'first')\n",
    "#print(analysis_genre.shape) #will print the shape of data frame\n",
    "#analysis_genre\n",
    "\n",
    "#analysis_genre.listed_in.to_list()    prints out the list of genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting listed_in column as index\n",
    "genre_pattern = analysis_genre.set_index('listed_in')\n",
    "\n",
    "#getting popular genre descriptions like comedies, Action and adventures, Horror movies and Romantic\n",
    "comedies_pattern = genre_pattern.loc[['Comedies']]\n",
    "Action_Adventures_pattern =  genre_pattern.loc[['Action & Adventure']]\n",
    "Horror_pattern = genre_pattern.loc[['Horror Movies']]\n",
    "Romantic_pattern = genre_pattern.loc[['Romantic']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining a function to get popular words from thd description of that specific genre \n",
    "#nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def get_pattern(x):\n",
    "    text = \" \".join(x['description'])  #joining all the values of Description COLUMN as a text\n",
    "    \n",
    "    # using word_tokenize() for splitting strings into tokens (nominally words). \n",
    "    #It splits tokens based on white space and punctuation. For example, commas and periods are taken as separate tokens.\n",
    "    textwords = word_tokenize(text)       \n",
    "    \n",
    "    #setting stopwords \n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    stop_words.update(['series','finds'])  #updating stopwords because as these words might not have specifc meaning\n",
    "    exclud_punc=[]  #making a new list\n",
    "    finl_words=[]  #making a new list\n",
    "    for w in textwords:        #using a for loop in tokenized text\n",
    "        if w.isalpha():        # checking whether a character is an alphabet or not\n",
    "            exclud_punc.append(w.lower())    #making sure that all the strings are lower case and appeding to the new list and \n",
    "    for word in exclud_punc:                 # using for loop in the appended list\n",
    "        if word not in stop_words:          #checking if the words are contained in stop_words\n",
    "            finl_words.append(word)         #appedning the filtered words, which are not in stop words\n",
    "\n",
    "    return finl_words       #returning the appended list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comedy_word_list = get_pattern(comedies_pattern)    #calling a function to get popular words for comedy genre\n",
    "action_Adv_list = get_pattern(Action_Adventures_pattern) #calling a function to get popular words for action and adventure genre\n",
    "horror_list =get_pattern(Horror_pattern)  #calling a function to get popular words for horror genre\n",
    "romantic_list = get_pattern(Romantic_pattern) #calling a fucntion to get popular words for romantic genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#w1 = WordCloud(max_font_size=50, max_words=150, colormap=\"Oranges_r\").generate(horror_list_wc)\n",
    "#wordcloud2 = WordCloud().generate(action_Adv_wc)\n",
    "\n",
    "#making a list of the lists\n",
    "list_words = [horror_list,action_Adv_list, comedy_word_list, romantic_list]\n",
    "\n",
    "#making a list for the tile\n",
    "title=['horror genre','action and adventure genre','comedy genre','romantic genre']\n",
    "j =0   #setting the counter\n",
    "for i in list_words:   #using for loop in the lists\n",
    "    text = \" \".join(i)   #joining all the words from the list and making it like whole bag of words  \n",
    "    w1 = WordCloud(max_font_size=50, max_words=150, colormap=\"Oranges_r\").generate(text)  #generating wordcloud\n",
    "    plt.figure(figsize = (10, 8))\n",
    "    plt.imshow(w1)\n",
    "    plt.title(f\"Popoular words for {title[j]}\", fontsize=20)  #setting title\n",
    "#plt.imshow(wordcloud2)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show() #plotting\n",
    "    j= j+1  #increasing counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pattern for sentiments of title\n",
    "\n",
    "I have seen some people who are only tempted to see the movie which have positive sentiments. Like my aunt only watches the movie which gives her positive influence. She reads the description of movie and if the movie description has word like violence (negative vibes), she will ignore that. So lets analyze the sentiment of the description of title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting specific columns from the dataframe\n",
    "copy_count_rated_country\n",
    "synopsis_analysis = copy_count_rated_country.loc[:,['description', 'rating_num','rating_standard','rating']]\n",
    "synopsis_analysis = synopsis_analysis.set_index(['description'])  #setting index\n",
    "synopsis_analysis = synopsis_analysis.reset_index()  #resetting index\n",
    "synopsis_analysis.head(2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making a new list\n",
    "value_list =[]  \n",
    "sentiment_list=[]\n",
    "\n",
    "#using for loop to iterate\n",
    "for each in range(len(synopsis_analysis)):\n",
    "    try:\n",
    "        text = synopsis_analysis['description'][each]  #getting specific value of description column \n",
    "        analysis = TextBlob(text)   #using python library to process the textual data.\n",
    "        value = analysis.sentiment.polarity  #analyzing the sentiment of text\n",
    "        value_list.append(value)\n",
    "        if value > 0:                  #setting the condition if the sentiment is greater than 0 to be positive\n",
    "            sentiment = 'positive'\n",
    "        elif value == 0:           #setting the condition if the sentiment is 0 to be neutral\n",
    "            sentiment ='neutral'\n",
    "        else:                        #setting the condition if the sentiment is lesser than 0 to be negative\n",
    "            sentiment ='negative'\n",
    "        sentiment_list.append(sentiment)  #appending sentiment values i.e. either positive, neutral or negative to the list\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "#print(value_list)  #prints the list\n",
    "#print(len(sentiment_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making the list into the column of dataframe\n",
    "synopsis_analysis['sentiment'] = sentiment_list\n",
    "\n",
    "#dropping nan values of the dataframe\n",
    "sentiment_analysis = synopsis_analysis.dropna()\n",
    "\n",
    "\n",
    "# calculating counts of sentiment movies/show \n",
    "count_sentiments = sentiment_analysis.sentiment.value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making a function to calculate the percent of sentiment values(i.e. either positive/neutral/negative)\n",
    "def get_sentiment_count(df):\n",
    "    y_count = df.sentiment.value_counts().reset_index()  #counting the values and restting the index\n",
    "    y_count =y_count.rename(columns={'index': \"sentiment_value\",\"sentiment\":\"counts\"})  #renaming the columns\n",
    "    total = sum(y_count['counts'])  #calcualting the sum \n",
    "    y_count['percent'] = y_count['counts'].apply(lambda x: (x/total)*100 )  #applying the percent function using lambda\n",
    "    return y_count #returning the dataframe\n",
    "\n",
    "high_count = get_sentiment_count(sentiment_analysis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting the percentage of sentiment for high-rated movies\n",
    "fig = go.Figure(data=[go.Pie(labels=high_count['sentiment_value'], values=high_count['percent'], hole=.3)])\n",
    "\n",
    "fig.update_layout(\n",
    "    title_text=\"Synopsis sentiment of high-rated movies\",\n",
    "    # Add annotations in the center of the donut pies.\n",
    "    annotations=[dict(text='sentiments', x=0.50, y=0.5, font_size=14, showarrow=False)])\n",
    "\n",
    "fig.show()\n",
    "\n",
    "#this is good news for people like my aunt, as she prefers to watch the movie that has positive sentiments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GETTING sentiment data for high-rated\n",
    "high_rated = sentiment_analysis[sentiment_analysis.rating_standard == 'High-rated']\n",
    "\n",
    "#getting sentiment data for low-rated\n",
    "low_rated = sentiment_analysis[sentiment_analysis.rating_standard == 'Low-rated']\n",
    "\n",
    "#dataframe with high positive sentiments among high-rated movies\n",
    "only_high_pos = high_rated[high_rated['sentiment'] == 'positive']\n",
    "\n",
    "#dataframe with high negative sentiments among high-rated movies\n",
    "only_high_neg = high_rated[high_rated['sentiment'] == 'negative']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finding popular words to look for high-rated movie/shows with positive sentiments\n",
    "high_pos_words = get_pattern(only_high_pos)    #calling a previous function\n",
    "\n",
    "#finding popular words to look for high-rated movie/shows with negative sentiments\n",
    "high_neg_words = get_pattern(only_high_neg)#calling a previous function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting word cloud that shows the popoular words to look to find high-rated positive sentiment movie/show\n",
    "text = \" \".join(high_pos_words)  #joining all the the words\n",
    "\n",
    "stopwords = set(STOPWORDS)  #setting stopwords\n",
    "stopwords.update([\"series\",'show'])  #updating stopwords with series and show as these words might not be that significant\n",
    "#creating a word cloud image\n",
    "wc = WordCloud(stopwords =stopwords, background_color='white',max_words =1000)\n",
    "#generate a wordcloud\n",
    "wc.generate(text)\n",
    "\n",
    "#show\n",
    "plt.figure(figsize=[15,7])\n",
    "plt.title(\"Positive words to find high-rated movies with positive-sentiment\", fontsize = 30)\n",
    "\n",
    "plt.imshow(wc)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommendation according to title, type, cast, director, listed_in "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using countvectorizer and cosine similarity to provide some recommendations according to title, type, cast, director and genre.\n",
    "Countvectorizer is used to transform a given text into a vector on the basis of the frequency (count) of each word that occurs in the entire text. And ‘cosine_similarity’ is used to find the similarity. Using cossine similarity means to calculate the cosine of the angle between two vectors. It does not mean finding straight line distance between two points "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting specific columns from the dataframe\n",
    "new_df_recommend = n_data_filter.loc[:,['type','title','director','cast','listed_in']]\n",
    "new_df =new_df_recommend.copy()  #making a copy of the dataframe\n",
    "new_df = new_df.fillna('') #filling na values \n",
    "#new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining a function to clean data according to the need\n",
    "def clean_data(x):\n",
    "#Check if string exists. If not, return empty string\n",
    "    if isinstance(x, str): \n",
    "        return str.lower(x.replace(\" \", \"\")) #making string lowercase and replacing whitespace\n",
    "    else:\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using for loop and calling the function to clean the data\n",
    "columns = ['type','title','director','cast','listed_in']\n",
    "for col in columns:\n",
    "    new_df[col] = new_df[col].apply(clean_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining function that comines all the columns together\n",
    "def combine_cols(x):\n",
    "    return ''.join(x['type']) + ' ' + ''.join(x['title']) + ' ' + x['director'] + ' ' + ''.join(x['cast'])+' ' + '' .join(x['listed_in'])\n",
    "\n",
    "#applying the defined functions\n",
    "new_df['combined_cols'] = new_df.apply(combine_cols , axis=1)\n",
    "# new_df['combined_cols']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making another recommend function  using countvectorizer \n",
    "def recommend_func(x,user_title,new_df_recommend):\n",
    "    \n",
    "    #calling countvectorizer and using stop_words functionality to remove the unwanted words\n",
    "    count_convert_vector = CountVectorizer(stop_words='english')\n",
    "    \n",
    "    #Convert a collection of text documents to a matrix of counts\n",
    "    count_convert_vector_matrix = count_convert_vector.fit_transform(x['combined_cols'])\n",
    "\n",
    "    # Compute the Cosine Similarity matrix based on the matrix of counts\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "    #calling a cosine_similarity function and passing the counts \n",
    "    similarity_metric = cosine_similarity(count_convert_vector_matrix, count_convert_vector_matrix)\n",
    "\n",
    "    # Reset index of your main DataFrame and construct reverse mapping as before\n",
    "    try:\n",
    "        x = x.reset_index()\n",
    "        index_col = pd.Series(x.index, index=x['title'])\n",
    "    except:\n",
    "        index_col = pd.Series(x.index, index=x['title'])\n",
    "    \n",
    "    \n",
    "    ################      recommendations_title_by_genre(user_title, similarity_metric)\n",
    "    user_title =user_title.replace(' ','').lower()\n",
    "\n",
    "    # Get the index of the movie that matches the title\n",
    "    index_val = index_col[user_title]\n",
    "    \n",
    "    print(\"The 5 shows related to your title are:\\n\")\n",
    "\n",
    "\n",
    "    # Get the pairwsie similarity scores of all movies with that movie\n",
    "    similarity_scores = enumerate(similarity_metric[index_val])\n",
    "\n",
    "\n",
    "    # Sort the movies based on the similarity scores\n",
    "    #using the sort function to filter the scores and arrange them in descending order\n",
    "    similarity_scores = sorted(similarity_scores, key=lambda a: a[1], reverse=True)\n",
    "    \n",
    "    # Get the scores of the 5 most similar movies\n",
    "    similarity_scores = similarity_scores[1:6]\n",
    "    \n",
    "    \n",
    "   \n",
    "    # Get the movie indices\n",
    "    movie_index_val=[]\n",
    "    #usig for loop to get the scores and appending it to new list \n",
    "    for similar in similarity_scores:\n",
    "        value = similar[0]\n",
    "        movie_index_val.append(value)\n",
    "    \n",
    "\n",
    "    # Return the top 5 most similar movies\n",
    "    my_list = new_df_recommend['title'].iloc[movie_index_val]\n",
    "    for my in my_list:  #using for loop to print one by one\n",
    "        print(my)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining another function to get the title that the user chose\n",
    "def get_df_title_genre(new_df,user_genre, user_title):\n",
    "    #replacing the whitespaces because our all the titles and other features are combined and removed whitespaces for processing\n",
    "    user_genre = user_genre.replace(' ','').lower()\n",
    "    #getting data frame with specific genre from listed_in columns\n",
    "    x = new_df[new_df.listed_in == user_genre]\n",
    "    \n",
    "    #calling the function and passing the dataframe, title and genre to the function\n",
    "    recommend_func(x,user_title,new_df_recommend)\n",
    "\n",
    "def get_df_title(new_df,user_title): #defining the function that will get the title \n",
    "    x = new_df.copy()  #making the new dataframe\n",
    "    recommend_func(x,user_title,new_df_recommend)#calling the function and passing the title name, data frame \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_genre(user_genre): #defining the function to let the user choose genre\n",
    "    \n",
    "    # getting the specific genre according to user choice\n",
    "    movie_names =new_df_recommend[new_df_recommend['listed_in']== user_genre]\n",
    "    print(user_genre) #printing the user genre choice\n",
    "    name_list = movie_names['title'].to_list() #getting the title list of titles from the genre chose\n",
    "    print(name_list[0:3]) #printing three title names based on the genre chose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining the menu function \n",
    "def menu():\n",
    "    while True:\n",
    "        #getting the input from users\n",
    "        inp = input(\"How would you like to get the recommendations? \\n\"\n",
    "                        \"      Enter 'T' if you would like to get recommendation just by title:\\n\"\n",
    "                        \"      Enter 'G' if you would like to get recommendation with genre and title:\\n\"\n",
    "                        \"      Enter 'N' for no recommendations and to quit:\\n\").upper()\n",
    "\n",
    "        if inp == 'T': #setting the condition\n",
    "            user_input=input(\"Enter the title name to get some recommendation: \")  #asking the input\n",
    "            \n",
    "            print(f\"Your title is:{user_input} \") #printing the title or the input that the user entered\n",
    "            print()  #leaving one line space\n",
    "            #print(\"The 5 shows related to your title are: \\n\") #print prompt\n",
    "            get_df_title(new_df,user_input) #calling the function that passes the user title and dataframe\n",
    "\n",
    "        elif inp =='G':  #setting another condition\n",
    "            print(\"Genre options:\\n\"  #printing the prompt\n",
    "                  \"    (Enter) 'A'for 'Documentaries'\\n\"\n",
    "                  \"    (Enter) 'B' for 'Stand-Up Comedy'\\n\"\n",
    "                  \"    (Enter) 'C' for 'Dramas, Independent Movies, International Movies'\\n\"\n",
    "                  '    (Enter) \"D\" for \"Kids\\' TV \"\\n'\n",
    "                  \"    (Enter) 'E' for 'Dramas, International Movies, Romantic Movies'\\n\"\n",
    "                  \"    (Enter) 'F' for 'Action & Adventure, Sci-Fi & Fantasy'\\n\"\n",
    "                  \"    (Enter) 'G' for 'Horror Movies, Thrillers'\\n\")\n",
    "                 \n",
    "\n",
    "            user_genre = input(\"Enter the letter for the genre you would like:\\n \").upper() # prompt for user input\n",
    "            list_options = ['A','B','C','D','E','F','G']  #setting the list with user options\n",
    "            \n",
    "            #list of genre options\n",
    "            genre_options = ['Documentaries','Stand-Up Comedy','Dramas, Independent Movies, International Movies', \"Kids\\' TV\",\n",
    "                             'Dramas, International Movies, Romantic Movies','Action & Adventure, Sci-Fi & Fantasy','Horror Movies, Thrillers']\n",
    "            #setting the condition and checking if the user options is in list options\n",
    "            if user_genre in list_options:\n",
    "                i = list_options.index(user_genre)   #getting the index of user genre\n",
    "                genre_name = genre_options[i]      \n",
    "                print(f\"You selected {genre_name} genre.\\n\")   #printing the genre name\n",
    "                choose_genre(user_genre = genre_name) #calling the function that will choose genre\n",
    "            else:\n",
    "                print()\n",
    "                #print prompt\n",
    "                print(\"You should select the above genre options. Please select genre from above options\")\n",
    "                menu() #if the user did not enter the input from the menu provided call the menu function again\n",
    "\n",
    "            user_title =input(\"Enter the title name to get some recommendation like that: \") #getting title input\n",
    "            print()\n",
    "        \n",
    "            \n",
    "            try:\n",
    "                get_df_title_genre(new_df,genre_name, user_title) #calling the title function passing the user chose title and the dataframe\n",
    "            except Exception:\n",
    "                print(\"The title could not be found. Please enter other title name.\")\n",
    "                menu()\n",
    "\n",
    "\n",
    "        elif inp == 'N':  #setting condition to validate the user choice\n",
    "            return \"Thank you!!!!!\"\n",
    "            break  #breaking the loop if user entered the invalid input\n",
    "            \n",
    "        \n",
    "        else:\n",
    "            print(\"Please select the correct options.\")\n",
    "            continue  #continuing the loop to provide the user to continue with the program\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "menu()  #calling the function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting Rating according to classification (machine learning technique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#n_data_filter.head(1)  #taking a glance of the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting specific columns from the dataframe\n",
    "data_knn = n_data_filter.loc[:,['netflixid','title','director','cast','rating']]\n",
    "\n",
    "#dropping the nan values from the dataframe\n",
    "data_knn = data_knn.dropna()\n",
    "#data_knn   #looking how the dataframe looks like"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I want to predict the rating on based on cast, director, and rating category (TVMA, TV-14) of the movie/show. This technique is also called the supervised machine learning because we actually know what we want to predict. We know that all the features we are going to choose is in string format to we have to convert them to binary for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting the cast names to binary\n",
    "\n",
    "#making a empty list\n",
    "castList = []\n",
    "\n",
    "#using the for loop to loov over the cast column of dataframe to get the speciifc cast names\n",
    "for ind, record in data_knn.iterrows():\n",
    "    cast = record[\"cast\"]\n",
    "    \n",
    "    # getting each cast names froma all the cast list\n",
    "    for i in cast:\n",
    "        #validating if the cast name is in the list because we do not want the same cast name to be repeated\n",
    "        if i not in castList:  \n",
    "            castList.append(i)  #only appending the cast name that is not in the list \n",
    "\n",
    "\n",
    "#print(castList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# once we have each cast name we have to convert it to 1 or 0's\n",
    "#so we define the function called convert\n",
    "def convert(x_list, data_list):\n",
    "    \n",
    "    #made a new list so that we will append it later\n",
    "    convert_to_binary=[]\n",
    "    \n",
    "    #using for loop from the list passed \n",
    "    for y in data_list:\n",
    "        if y in x_list:  #setting condition to check if the list contains the items\n",
    "            convert_to_binary.append(1) #so if the item was in the list convert that item to 1\n",
    "        else:\n",
    "            convert_to_binary.append(0)  # if the item was not in the list convert it to 0.\n",
    "        \n",
    "    return convert_to_binary #returning the list\n",
    "\n",
    "#applying the function for cast column of dataframe because we wanted to convert the cast column to binary\n",
    "data_knn['cast_binary'] = data_knn['cast'].apply(lambda x: convert(x,castList)) \n",
    "data_knn['cast_binary'].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making a director list so that we will have director names and append it to this list\n",
    "directorList=[]\n",
    "for i in data_knn['director']:  #using for loop in director columns\n",
    "    if i not in directorList:  #validating if the director name is not repeated\n",
    "        directorList.append(i)   #appending the director names in the list\n",
    "        \n",
    "        \n",
    "#calling a function and making a new column in dataframe with 1 and 0 values      \n",
    "data_knn['director_binary'] = data_knn['director'].apply(lambda x: convert(x, directorList))\n",
    "data_knn.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#also we are using rating category like: TV-MA, TV-14 to predict the rating so\n",
    "#making a new description list\n",
    "\n",
    "rating_list = []\n",
    "\n",
    "#using for loop to get the row and column value of the dataframe\n",
    "for ind, record in data_knn.iterrows():\n",
    "    x = record[\"rating\"]\n",
    "    \n",
    "    #using for loop to get each rating category\n",
    "    for each in x:\n",
    "        if each not in rating_list: #validating that the rating category is not repeated for same record\n",
    "            rating_list.append(each)\n",
    "\n",
    "# calling the function to convert it to binary and making a new column \n",
    "data_knn['rating_binary'] = data_knn['rating'].apply(lambda x: convert(x,rating_list))\n",
    "data_knn.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spatial is python module is used to find the distance between two points. So when we imported spatial and used cosine distance modeule, it finds the distance between two points based on cosine. Cossine distance is based on cosine similarity. Cosine similarity is finding the distance based on cos-angle drawn by those two points. For example, if two points lie in same vector or nearer, the distance between them is nominal which increases the similarity or vice-versa. Thus this concept is applied to find the recommendation system as it finds the similarity between the features (casting,director and so on). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "#defining the fuunction that finds the similarity between two features\n",
    "def Similarity(firstid, secondid):\n",
    "    #getting specific rows\n",
    "    first_set = data_knn.iloc[firstid] \n",
    "    second_set = data_knn.iloc[secondid]\n",
    "    \n",
    "    #getting the column data of cast binary of two sets\n",
    "    first_cast = first_set['cast_binary']\n",
    "    second_cast = second_set['cast_binary']\n",
    "    \n",
    "   # Compute the Cosine distance of 1-D array between first_cast and second_cast\n",
    "    cast_distance = spatial.distance.cosine(first_cast, second_cast)\n",
    "    \n",
    "    \n",
    "    #getting the column data of director binary of two sets\n",
    "    first_director = first_set['director_binary']\n",
    "    second_director = second_set['director_binary']\n",
    "    \n",
    "    # Compute the Cosine distance of 1-D array between first_director and second_director\n",
    "    director_distance = spatial.distance.cosine(first_director, second_director)\n",
    "    \n",
    "    #getting the column data of rating binary of two sets\n",
    "    first_rating = first_set['rating_binary']\n",
    "    second_rating = second_set['rating_binary']\n",
    "    \n",
    "    # Compute the Cosine distance of 1-D array between first_rating and second_rating\n",
    "    rating_distance = spatial.distance.cosine(first_rating, second_rating)\n",
    "    \n",
    "    return director_distance + cast_distance + rating_distance # returns total distance between the two data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for example distance bewteen first row and hundreth row is calculated\n",
    "Similarity(0,99)\n",
    "\n",
    "# to understand more.....the data for the particular rows are printed\n",
    "print(data_knn.iloc[0])\n",
    "print(data_knn.iloc[99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting specific columns\n",
    "rating_view_to_merge = netflix_data_filter.loc[:,['netflixid','rating']]\n",
    "\n",
    "#renaming column names\n",
    "rating_view_to_merge.rename(columns={'rating':'rating_num'}, inplace =True)\n",
    "\n",
    "#mergind two dataframes\n",
    "merge_for_recommendation = pd.merge(data_knn, rating_view_to_merge, on='netflixid', how='inner')\n",
    "\n",
    "#print(merge_for_recommendation.shape) \n",
    "merge_for_recommendation.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specifying range into new variable\n",
    "new_id = range(0,merge_for_recommendation.shape[0])\n",
    "\n",
    "#making a new column with customized sequential ids\n",
    "merge_for_recommendation['new_id']=new_id\n",
    "\n",
    "#specifying \n",
    "merge_for_recommendation=merge_for_recommendation.drop(columns = ['netflixid'])\n",
    "\n",
    "#'original_title','genres','vote_average','genres_bin','cast_bin','new_id','director','director_bin','words_bin']]\n",
    "\n",
    "\n",
    "merge_for_recommendation.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ind, merge_for_recommend in merge_for_recommendation.iterrows():\n",
    "    print(merge_for_recommend)\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_for_recommendation = merge_for_recommendation.dropna()\n",
    "merge_for_recommendation.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNeighbors(standard_movie_to_comp, K):\n",
    "        distances = []\n",
    "        \n",
    "        #using for loop to acess the index no of rows and the data of rows with the columnnames\n",
    "        for ind, merge_for_recommend in merge_for_recommendation.iterrows():\n",
    "            \n",
    "            #checking if the value of id of series is matched with the user entered value of the series\n",
    "            if merge_for_recommend['new_id'] != standard_movie_to_comp['new_id'].values[0]:\n",
    "                \n",
    "                #calling similarity function to find the disance between the user selected title and the title located in dataframe\n",
    "                dist = Similarity(standard_movie_to_comp['new_id'].values[0], merge_for_recommend['new_id'])\n",
    "                \n",
    "                #appending the value of distances \n",
    "                distances.append((merge_for_recommend['new_id'], dist))\n",
    "        \n",
    "        #sorting the distances, constructing iterable object and fetching the 1st element out of it.\n",
    "        distances.sort(key=operator.itemgetter(1))\n",
    "        neighbors = []  #empty list\n",
    "    \n",
    "        #using for loop to append the distances \n",
    "        for x in range(K):\n",
    "            neighbors.append(distances[x])\n",
    "        return neighbors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_rating():\n",
    "    # asking title from the user\n",
    "    #using try and except block\n",
    "    try:\n",
    "        name_movie = input('Enter a movie title: ')\n",
    "        print('')\n",
    "\n",
    "        #looking for the users title in the dataframe. Once the title is found picking whole row and converting it to dataframe and transposing it\n",
    "\n",
    "        new_movie = merge_for_recommendation[merge_for_recommendation['title'].str.contains(name_movie)].iloc[0].to_frame().T\n",
    "        print('Selected Movie: ',new_movie.title.values[0])\n",
    "        \n",
    "    \n",
    "        K = 5 #supposing value of k to be 5 and Rating to be 0\n",
    "        Rating = 0\n",
    "        neighbors = getNeighbors(new_movie, K)  #calling function that will get the user input and title and find the distance\n",
    "\n",
    "\n",
    "        for i in neighbors:  #using for loop to find the rating by summing all the rating distances\n",
    "            Rating = Rating+merge_for_recommendation.iloc[i[0]][7] \n",
    "        \n",
    "        print('\\n')\n",
    "        Rating = Rating/K  #the predicted rating\n",
    "        print('The predicted rating for %s is %f' %(new_movie['title'].values[0],Rating))\n",
    "        print('The actual rating for %s is %f' %(new_movie['title'].values[0],new_movie['rating_num']))\n",
    "    \n",
    "        #print(f\"The predicted rating for {new_movie['title'].values[0]} is {Rating}\")\n",
    "        #print(f\"The actual rating for {new_movie['title'].values[0]} is {new_movie['rating_num']}\")\n",
    "\n",
    "    \n",
    "    except Exception:\n",
    "        print(\"\\nYour movie can not be processed/found for rating prediction. Please enter another movies like \\n\"\n",
    "                     \"Hold the Dark\\n\"\n",
    "                    \"Little Evil\")\n",
    "        predict_rating()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_rating()  #calling the function\n",
    "\n",
    "#try the following:\n",
    "    #Hold the Dark\n",
    "    #Gaga: Five Foot Two\n",
    "    # Time Trap\n",
    "    #Supergirl\n",
    "    #Little Evil\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN classifications for rating by director"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping duplicates\n",
    "n_data_filter =n_data.drop_duplicates(subset=['netflixid','title'])\n",
    "netflix_data_filter =netflix_dat.drop_duplicates(subset=['netflixid','title'])\n",
    "\n",
    "print(netflix_data_filter.duplicated().any()) #again checking if any duplicates left\n",
    "print(n_data_filter.duplicated().any()) # checking andy duplicates "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "copy_count_rated.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting specific columns\n",
    "rating_merge_classify = copy_count_rated.loc[:,['netflixid','rating_standard','rating_num']]\n",
    "#merging two dataframes\n",
    "merge_for_classification = pd.merge(data_knn, rating_merge_classify, on='netflixid', how='inner')\n",
    "\n",
    "merge_for_classification.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge_for_classification.director.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LabelEncoder is a utility class to help normalize labels such that they contain only values between 0 and n_classes-1 \n",
    "from sklearn import preprocessing\n",
    "\n",
    "#creating labelEncoder, this will encode the y(target) variables\n",
    "le = preprocessing.LabelEncoder()\n",
    "\n",
    "# Converting string labels into numbers.\n",
    "rating_encoded=le.fit_transform(merge_for_classification['rating'])\n",
    "merge_for_classification['rating_encoded'] = rating_encoded\n",
    "casting_encoded = le.fit_transform(merge_for_classification['cast'])\n",
    "merge_for_classification['casting_encoded'] = casting_encoded\n",
    "director_encoded = le.fit_transform(merge_for_classification['director'])\n",
    "merge_for_classification['director_encoded'] = director_encoded\n",
    "rating_standard_encoded = le.fit_transform(merge_for_classification['rating_standard'])\n",
    "merge_for_classification['rating_standard_encoded'] = rating_standard_encoded\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge_for_classification\n",
    "#.rating_standard_encoded.to_list()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining x values and y values\n",
    "X = merge_for_classification[['casting_encoded','director_encoded']].values\n",
    "y= merge_for_classification['rating_standard_encoded'].to_numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def classify_rating(c,d):\n",
    "    try:\n",
    "        X = merge_for_classification[['casting_encoded','director_encoded']].values\n",
    "        y= merge_for_classification['rating_standard_encoded'].values\n",
    "\n",
    "        #print(X)\n",
    "        #print(y)\n",
    "\n",
    "        # Split dataset into training set and test set\n",
    "        X_train_first, X_test_first, y_train_first, y_test_first = train_test_split(X, y, train_size = 0.7,test_size=0.3, random_state=4) # 70% training and 30% test\n",
    "\n",
    "\n",
    "        from sklearn.neighbors import KNeighborsClassifier\n",
    "        # Create the knn model.\n",
    "        # Look at the five closest neighbors.\n",
    "        knn = KNeighborsClassifier(n_neighbors=5)\n",
    "        # Fit the model on the training data.\n",
    "        knn.fit(X_train_first, y_train_first)\n",
    "        # Make point predictions on the test set using the fit model.\n",
    "        #predictions = knn.predict(X_test_first)\n",
    "\n",
    "        predicted= knn.predict([[c,d]]) \n",
    "\n",
    "\n",
    "        #print(predicted)\n",
    "        #print(metrics.accuracy_score(y_test_first, predicted))\n",
    "\n",
    "        print(\" \")\n",
    "\n",
    "        director_name = merge_for_classification[merge_for_classification['director_encoded'] == d].iloc[0][2]\n",
    "        movie_title = merge_for_classification[merge_for_classification['casting_encoded'] == c].iloc[0][1]\n",
    "\n",
    "        # checking the predictions\n",
    "        if predicted == [1]:\n",
    "            print(f\"If '{director_name}' directs '{movie_title}', the rating standard will be Low-rated.\")\n",
    "        else:\n",
    "            print(f\"If '{director_name}' directs '{movie_title}', the rating standard will be High-rated.\")\n",
    "\n",
    "    except IndexError:\n",
    "        print(\"Could not find the director name. Please enter another director name.\")\n",
    "        classify_rating(c,d)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_cast_director_info():\n",
    "    try:\n",
    "        title = input(\"Enter a title name: \")\n",
    "        print(\" \")\n",
    "\n",
    "        #getting title according to user\n",
    "        new_title = merge_for_classification[merge_for_classification['title'].str.contains(title)].iloc[0].to_frame().T\n",
    "\n",
    "        #getting encoded values from the user selection\n",
    "        cast_encoded = new_title.casting_encoded.values[0]\n",
    "        print(f\"The cast name for this movie are: \\n------{new_title.cast.values[0]}\")\n",
    "        print(\" \")\n",
    "        print(f\"This movie/show is {new_title.rating_standard.values[0]}.\")\n",
    "        print(\" \")\n",
    "        print(f\"The director name is {new_title.director.values[0]}\")\n",
    "\n",
    "        #asking for the director\n",
    "\n",
    "        director =input(\"Enter a director name that you would like to see this movie directed: \")\n",
    "        #new_movie = merge_for_classification[merge_for_classification['cast'].str.contains(name)].iloc[0].to_frame().T\n",
    "\n",
    "        #getting movie data from director\n",
    "        n_movie = merge_for_classification[merge_for_classification['director'].str.contains(director)].iloc[0].to_frame().T\n",
    "\n",
    "        #getting director encoded\n",
    "        direct_encoded = n_movie.director_encoded.values[0]\n",
    "\n",
    "        return cast_encoded, direct_encoded\n",
    "    except:\n",
    "        print(\"You might have entered different title or director name. Not found!!\")\n",
    "        get_cast_director_info()\n",
    "\n",
    "# Taissa Farmiga, Ben Rosenfield, Lindsay Burdge, Joshua Leonard, Jennifer Lafleur, \n",
    "#Peter Vack, Dana Wheeler-Nicholson, Jason Newman, Molly McMichael'\n",
    "# Jay Karas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#calling a function\n",
    "cast,director = get_cast_director_info()\n",
    "classify_rating(cast, director)\n",
    "# Title: Arrow, Jay Karas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"KNN_1.png\">\n",
    "\n",
    "<img src =\"KNN_2.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Findings and Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. From the data I collected, Netflix has more number of low-rated movies than high-rated movies. This means that there are ways where Netflix can increase more viewers. \n",
    "\n",
    "2. Except UK,many other countries like USA have high number of low-rated movies so Netflix has to emphasize on the country's population to impove it's rating standard of the content. So, to increase it's viewers, either netflix has to increase the genre contents that are less in numbers like scifi and fantasy, romantic; or focus on specific genre which have more low rated movies and try to improve the quality of them rather than genre which have abundant high-rated movies. For example there are more no of comedies genre but more of them are low -rated.So rather than emphasizing on drama genre,netflix in US has to focus on comedies, Kids and Teens. \n",
    "\n",
    "3. Netflix has more count of TV-MA, TV-14 , TV-PG and these top three ratung types are mostly adult type movies. Thus to increase its viewers, netflix have to increase its variability in contents. \n",
    "\n",
    "4. The duration of movie doesn't seem to have direct relation with rating. Mostly the movies are ranged within 60 mins to 140 mins.\n",
    "\n",
    "5. Getting content directed by Shannon Hartman, Jay Karas, Lilly Wachowski, Lana Wachowski, Mike Clattenburg can increase the viewers of netflix.\n",
    "\n",
    "6. There is some pattern on how netflix describes its content on basis of it's genre\n",
    "\n",
    "7. Based on classification technique, Netflix should try to predict its rating before releasing the content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lessons Learned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, I have tried to sum up all the lessons learned through the whole semester. I learned how to get data from API. I learned how to filter and process the large datasets. Most importantly, I learned some concept of supervised learning (KNN classification, using cosine similarity) and some ways of applying it to provide some recommendations. Obviously, there are always ways of makin it better, so I will keep learning new techniques to polish my calibers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My github.io link is: https://ujjoli.github.io/Netflix-analysis-and-suggestion/\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thank you!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
